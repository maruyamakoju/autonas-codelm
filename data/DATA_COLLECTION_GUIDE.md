# Python Corpus Data Collection Guide

**ç›®çš„**: 100MB-1GBè¦æ¨¡ã®Pythonã‚³ãƒ¼ãƒ‰ã‚’åé›†ã—ã¦ã€char-levelè¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ã†

---

## ğŸ¯ åé›†ç›®æ¨™

| è¦æ¨¡ | ã‚µã‚¤ã‚º | æœŸå¾…åŠ¹æœ |
|------|--------|---------|
| **æœ€å°** | 50MB | ãƒ¢ãƒ¼ãƒ‰å´©å£ŠãŒéƒ¨åˆ†çš„ã«æ”¹å–„ |
| **æ¨å¥¨** | 100-500MB | å¤§å¹…ãªå“è³ªå‘ä¸Šã€coherentãªã‚³ãƒ¼ãƒ‰ç”Ÿæˆ |
| **ç†æƒ³** | 1GB+ | GPT-2 smallãƒ¬ãƒ™ãƒ«ã®å“è³ª |

**ç¾çŠ¶**: 1.3MB (data/code_char/train.txt) â†’ **100å€ä»¥ä¸Š**ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦

---

## ğŸ“¦ Option A: The Stack Dataset (æ¨å¥¨)

### æ¦‚è¦
- **æä¾›å…ƒ**: HuggingFace BigCode
- **URL**: https://huggingface.co/datasets/bigcode/the-stack
- **ã‚µã‚¤ã‚º**: Pythonéƒ¨åˆ†ã§æ•°GB-æ•°åGB
- **å“è³ª**: é«˜å“è³ªã€deduplicatedã€license-filtered

### æ–¹æ³•1: HuggingFace Datasets ãƒ©ã‚¤ãƒ–ãƒ©ãƒª (æ¨å¥¨)

```bash
# HuggingFace datasetsã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install datasets

# Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
cd data
python download_the_stack.py
```

**`download_the_stack.py` ã®å†…å®¹:**

```python
from datasets import load_dataset
from pathlib import Path

# The Stack ã® Python ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ (100MB ã‚µãƒ³ãƒ—ãƒ«)
print("Loading The Stack (Python)...")
ds = load_dataset(
    "bigcode/the-stack",
    data_dir="data/python",
    split="train",
    streaming=True  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã§å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‰±ã†
)

# 100MBåˆ†ã®Pythonãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
output_dir = Path("raw_python/the_stack")
output_dir.mkdir(parents=True, exist_ok=True)

total_bytes = 0
target_bytes = 100 * 1024 * 1024  # 100MB
file_count = 0

for example in ds:
    content = example['content']

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—
    file_path = output_dir / f"sample_{file_count:06d}.py"
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(content)

    total_bytes += len(content.encode('utf-8'))
    file_count += 1

    if file_count % 100 == 0:
        print(f"Collected {file_count} files, {total_bytes / 1024 / 1024:.1f} MB", end='\r')

    # ç›®æ¨™ã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰çµ‚äº†
    if total_bytes >= target_bytes:
        break

print(f"\nOK Downloaded {file_count} files, {total_bytes / 1024 / 1024:.2f} MB")
```

**å®Ÿè¡Œ:**

```bash
cd data
python download_the_stack.py

# å‡ºåŠ›: raw_python/the_stack/*.py (100MBåˆ†)
```

### æ–¹æ³•2: æ‰‹å‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```bash
# HuggingFace CLI ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
pip install huggingface-hub

# ãƒ­ã‚°ã‚¤ãƒ³ (åˆå›ã®ã¿)
huggingface-cli login

# The Stack Python ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
huggingface-cli download bigcode/the-stack --repo-type dataset --include "data/python/*.parquet" --local-dir data/raw_python/the_stack_parquet

# Parquet ã‚’ .py ãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ› (åˆ¥é€”ã‚¹ã‚¯ãƒªãƒ—ãƒˆå¿…è¦)
```

---

## ğŸ“¦ Option B: GitHub äººæ°—ãƒªãƒã‚¸ãƒˆãƒª (ç°¡å˜)

### æ¦‚è¦
- **æä¾›å…ƒ**: GitHub
- **ã‚µã‚¤ã‚º**: ãƒªãƒã‚¸ãƒˆãƒªã”ã¨ã«æ•°MB-æ•°åMB
- **å“è³ª**: é«˜å“è³ªã€å®Ÿéš›ã®ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ã‚³ãƒ¼ãƒ‰
- **æ¨å¥¨ãƒªãƒã‚¸ãƒˆãƒª**: Pythonç•Œã®æœ‰åãƒ©ã‚¤ãƒ–ãƒ©ãƒª

### ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ‰‹é †

```bash
cd data/raw_python

# Pythonæ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªé¢¨ã®äººæ°—ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
git clone https://github.com/psf/requests.git              # ~5MB
git clone https://github.com/pallets/flask.git             # ~3MB
git clone https://github.com/django/django.git             # ~15MB
git clone https://github.com/pytest-dev/pytest.git         # ~5MB
git clone https://github.com/numpy/numpy.git               # ~30MB (Cå«ã‚€)
git clone https://github.com/pandas-dev/pandas.git         # ~40MB
git clone https://github.com/scikit-learn/scikit-learn.git # ~30MB
git clone https://github.com/fastapi/fastapi.git           # ~3MB
git clone https://github.com/tornadoweb/tornado.git        # ~2MB
git clone https://github.com/boto/boto3.git                # ~5MB

# ã‚ˆã‚Šå°è¦æ¨¡ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
git clone https://github.com/kennethreitz/records.git
git clone https://github.com/jazzband/pip-tools.git
git clone https://github.com/pypa/pip.git
git clone https://github.com/pytestarch/pytestarch.git

# åˆè¨ˆ: 100MB+ é”æˆå¯èƒ½
```

**æ³¨æ„ç‚¹:**
- `.git/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ä¸è¦ (å‰Šé™¤ã—ã¦OK)
- ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚å«ã¾ã‚Œã‚‹ (ã‚€ã—ã‚å¤šæ§˜æ€§ãŒå¢—ãˆã¦è‰¯ã„)
- Cæ‹¡å¼µã‚„JSãŒæ··ã–ã£ã¦ã„ã‚‹ãƒªãƒã‚¸ãƒˆãƒªã‚‚ã‚ã‚‹ãŒã€`prepare_python_corpus.py` ãŒ `.py` ã ã‘ãƒ•ã‚£ãƒ«ã‚¿ã™ã‚‹

### .gitãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤ (å®¹é‡ç¯€ç´„)

```bash
cd data/raw_python
find . -name ".git" -type d -exec rm -rf {} +

# ã¾ãŸã¯ Windows ã®å ´åˆ:
# PowerShell ã§å„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã® .git ã‚’å‰Šé™¤
Get-ChildItem -Path . -Recurse -Directory -Filter ".git" | Remove-Item -Recurse -Force
```

---

## ğŸ“¦ Option C: CodeSearchNet Dataset

### æ¦‚è¦
- **æä¾›å…ƒ**: GitHub + Microsoft Research
- **URL**: https://github.com/github/CodeSearchNet
- **ã‚µã‚¤ã‚º**: Pythonéƒ¨åˆ†ã§ ~500MB
- **å“è³ª**: GitHubã‹ã‚‰åé›†ã€docstringä»˜ãã‚³ãƒ¼ãƒ‰å¤šã‚

### ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ‰‹é †

```bash
# CodeSearchNet ã® Python ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
cd data
wget https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/python.zip

# è§£å‡
unzip python.zip -d raw_python/codesearchnet

# JSONLã‹ã‚‰Pythonã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡º (åˆ¥é€”ã‚¹ã‚¯ãƒªãƒ—ãƒˆå¿…è¦)
```

**JSONLã‹ã‚‰ã‚³ãƒ¼ãƒ‰æŠ½å‡ºã‚¹ã‚¯ãƒªãƒ—ãƒˆ** (`extract_codesearchnet.py`):

```python
import json
from pathlib import Path

input_dir = Path("raw_python/codesearchnet/python/final/jsonl/train")
output_dir = Path("raw_python/codesearchnet_extracted")
output_dir.mkdir(parents=True, exist_ok=True)

file_count = 0
for jsonl_file in input_dir.glob("*.jsonl.gz"):
    import gzip
    with gzip.open(jsonl_file, 'rt', encoding='utf-8') as f:
        for line in f:
            entry = json.loads(line)
            code = entry.get('code', '')

            if code.strip():
                file_path = output_dir / f"sample_{file_count:06d}.py"
                with open(file_path, 'w', encoding='utf-8') as out:
                    out.write(code)
                file_count += 1

                if file_count % 1000 == 0:
                    print(f"Extracted {file_count} files...", end='\r')

print(f"\nOK Extracted {file_count} Python code samples")
```

---

## ğŸ”„ ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ•ãƒ­ãƒ¼

### 1. ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†å¾Œã®ç¢ºèª

```bash
cd data/raw_python

# Pythonãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ç¢ºèª
find . -name "*.py" | wc -l

# åˆè¨ˆã‚µã‚¤ã‚ºã‚’ç¢ºèª (Windows)
dir /s *.py | find "File(s)"

# åˆè¨ˆã‚µã‚¤ã‚ºã‚’ç¢ºèª (Unix)
find . -name "*.py" -exec du -ch {} + | grep total$
```

### 2. ã‚³ãƒ¼ãƒ‘ã‚¹ç”Ÿæˆ

```bash
cd nas

# raw_python â†’ code_char_big ã«å¤‰æ›
python scripts/prepare_python_corpus.py \
  --src_dir ../data/raw_python \
  --train_out ../data/code_char_big/train.txt \
  --val_out ../data/code_char_big/val.txt \
  --val_ratio 0.01

# çµæœç¢ºèª
ls -lh ../data/code_char_big/
# æœŸå¾…:
#   train.txt: 100MB-1GB
#   val.txt: 1MB-10MB
```

### 3. ã‚µã‚¤ã‚ºç¢ºèª

```bash
cd ../data/code_char_big

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
du -h train.txt val.txt

# è¡Œæ•°
wc -l train.txt val.txt

# æ–‡å­—æ•°
wc -m train.txt val.txt
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:**

```
train.txt: 123.45 MB, 1,234,567 lines
val.txt:   1.23 MB, 12,345 lines
```

---

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯

### Python ã‚³ãƒ¼ãƒ‰ã® sanity check

```python
# Check if train.txt contains valid Python patterns
import re

with open('../data/code_char_big/train.txt', 'r', encoding='utf-8') as f:
    sample = f.read(10000)  # æœ€åˆã®10KB

# æœŸå¾…ã•ã‚Œã‚‹Pythonãƒ‘ã‚¿ãƒ¼ãƒ³
patterns = [
    r'\bdef\s+\w+',       # é–¢æ•°å®šç¾©
    r'\bclass\s+\w+',     # ã‚¯ãƒ©ã‚¹å®šç¾©
    r'\bimport\s+\w+',    # importæ–‡
    r'\bfor\s+\w+\s+in',  # forãƒ«ãƒ¼ãƒ—
    r'\bif\s+.+:',        # ifæ–‡
]

print("=== Data Quality Check ===")
for pattern in patterns:
    matches = len(re.findall(pattern, sample))
    print(f"{pattern:30s}: {matches:3d} matches")

# æœŸå¾…: å„ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒæ•°å›ä»¥ä¸Šå‡ºç¾
```

---

## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ãƒ‡ãƒ¼ã‚¿åé›†ãŒå®Œäº†ã—ãŸã‚‰ â†’ [EXPERIMENTS.md Section 12](../nas/EXPERIMENTS.md#12-bigdata-training-ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µå®Ÿé¨“) ã«å¾“ã£ã¦å­¦ç¿’é–‹å§‹

```bash
cd nas

python train_best.py \
  --arch_json models/codenas_best_current.json \
  --experiment_name v1_bigdata_char \
  --train_path ../data/code_char_big/train.txt \
  --val_path ../data/code_char_big/val.txt \
  --max_steps 100000 \
  --log_dir logs/train_v1_bigdata_char \
  --device cuda:0
```

---

## ğŸ’¡ Tips

### ãƒ‡ãƒ¼ã‚¿åé›†ã®å„ªå…ˆé †ä½

1. **æœ€åˆ**: Option B (GitHub repos) ã§ 50-100MB é›†ã‚ã¦ãƒ†ã‚¹ãƒˆ
   - ç°¡å˜ã€é€Ÿã„ (git clone ã®ã¿)
   - å“è³ªãŒé«˜ã„
   - ã™ãã«å­¦ç¿’é–‹å§‹ã§ãã‚‹

2. **æ¬¡**: åŠ¹æœãŒã‚ã‚Œã° Option A (The Stack) ã§ 500MB-1GB
   - ã‚ˆã‚Šå¤§è¦æ¨¡ã€å¤šæ§˜æ€§é«˜ã„
   - ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«æ™‚é–“ã‹ã‹ã‚‹ãŒå“è³ªã¯ä¿è¨¼ã•ã‚Œã¦ã„ã‚‹

3. **ã‚ªãƒ—ã‚·ãƒ§ãƒ³**: CodeSearchNet ã¯ docstring ãŒè±Šå¯Œ
   - é–¢æ•°/ã‚¯ãƒ©ã‚¹ã®èª¬æ˜æ–‡ãŒå¤šã„
   - Code-to-Text ã‚¿ã‚¹ã‚¯ã«å‘ã„ã¦ã„ã‚‹

### ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã®ç›®å®‰

- raw_python (ç”Ÿãƒ‡ãƒ¼ã‚¿): 200MB-2GB
- code_char_big (å¤‰æ›å¾Œ): 100MB-1GB
- ãƒ­ã‚°ãƒ»ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: 100MB-500MB

**åˆè¨ˆ**: 500MB-4GB ã®ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’ç¢ºä¿

---

**Last Updated**: 2024-12-08
