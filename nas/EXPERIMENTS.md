# NAS Experiments Command Reference

ã‚ˆãä½¿ã†ã‚³ãƒãƒ³ãƒ‰é›†ã€‚æœªæ¥ã®è‡ªåˆ†ç”¨ã€‚

## Quick Reference

| ç›®çš„ | ã‚³ãƒãƒ³ãƒ‰ |
|------|---------|
| ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ | `python evolution.py --experiment_name smoke_test --population 4 --generations 2 ...` |
| ãƒŸãƒ‡ã‚£ã‚¢ãƒ å®Ÿé¨“ | `python evolution.py --experiment_name medium_nas --population 10 --generations 5 ...` |
| æœ¬ç•ªå®Ÿé¨“ | `.\scripts\run_dual_gpu_production.ps1` |
| Sanity Check | `bash scripts/sanity_check.sh` |
| ä¸¦åˆ—ãƒ­ã‚°è§£æ | `python analyze_parallel_stats.py --log_dir logs/exp/parallel` |
| GPUæ ¡æ­£ | `python calibrate_gpus.py` |

---

## 1. Smoke Test (å‹•ä½œç¢ºèªç”¨)

```bash
cd nas

# æœ€å°æ§‹æˆã§ã‚µã‚¯ãƒƒã¨å‹•ä½œç¢ºèª (~2-3åˆ†)
python evolution.py \
  --experiment_name "smoke_test" \
  --population 4 \
  --generations 2 \
  --use_real_training \
  --train_path "../data/code_char/train.txt" \
  --val_path "../data/code_char/val.txt" \
  --seq_len 64 \
  --batch_size 8 \
  --max_train_steps 50 \
  --device cuda:0 \
  --search_mode "minimal"
```

---

## 2. Medium Run (é–‹ç™ºãƒ»ãƒ‡ãƒãƒƒã‚°ç”¨)

```bash
# ä¸­è¦æ¨¡å®Ÿé¨“ (~30-40åˆ†)
python evolution.py \
  --experiment_name "medium_nas" \
  --population 10 \
  --generations 5 \
  --use_real_training \
  --train_path "../data/code_char/train.txt" \
  --val_path "../data/code_char/val.txt" \
  --seq_len 128 \
  --batch_size 16 \
  --max_train_steps 150 \
  --device cuda:0 \
  --search_mode "minimal"
```

---

## 3. Parallel Evaluation (Single GPU)

```bash
# ä¸¦åˆ—è©•ä¾¡ãƒ†ã‚¹ãƒˆï¼ˆ1GPUã€ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ï¼‰
python evolution.py \
  --experiment_name "parallel_test" \
  --population 8 \
  --generations 3 \
  --use_real_training \
  --train_path "../data/code_char/train.txt" \
  --val_path "../data/code_char/val.txt" \
  --seq_len 128 \
  --batch_size 16 \
  --max_train_steps 100 \
  --search_mode "minimal" \
  --parallel \
  --gpus "cuda:0"
```

---

## 4. Dual-GPU Experiments (5090 + 4090)

### Quick Test (~10-15åˆ†)
```powershell
cd nas/scripts
.\run_dual_gpu_quick_test.ps1
```

### Standard Run
```powershell
.\run_dual_gpu.ps1
```

### Production Run (ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŒ‡å®šå¯)
```powershell
.\run_dual_gpu_production.ps1 -Population 50 -Generations 30 -MaxTrainSteps 500
```

### ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ç›´æ¥å®Ÿè¡Œ
```bash
python evolution.py \
  --experiment_name "code_nas_dual_gpu_v1" \
  --population 40 \
  --generations 20 \
  --use_real_training \
  --train_path "../data/code_char/train.txt" \
  --val_path "../data/code_char/val.txt" \
  --seq_len 256 \
  --batch_size 32 \
  --max_train_steps 500 \
  --search_mode "medium" \
  --parallel \
  --gpus "cuda:0,cuda:1"
```

---

## 5. CodeNAS v1 (Production Single GPU)

**æœ¬ç•ªå®Ÿé¨“ç”¨è¨­å®šã€‚1GPUã§æœ¬æ ¼çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¢ç´¢ã‚’å®Ÿè¡Œã€‚**

### è¨­å®šå€¤
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ | èª¬æ˜ |
|-----------|-----|------|
| population | 24 | ä¸–ä»£ã‚ãŸã‚Šã®å€‹ä½“æ•° |
| generations | 8 | é€²åŒ–ä¸–ä»£æ•° |
| search_mode | medium | æ¢ç´¢ç©ºé–“ï¼ˆã‚ˆã‚Šå¤šæ§˜ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰ |
| max_train_steps | 300 | å„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•° |
| seq_len | 256 | ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· |
| batch_size | 32 | ãƒãƒƒãƒã‚µã‚¤ã‚º |
| device | cuda:0 | ä½¿ç”¨GPU |

### PowerShellã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
```powershell
cd nas\scripts
.\run_codenas_v1_single.ps1
```

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŒ‡å®šå®Ÿè¡Œ
```powershell
.\run_codenas_v1_single.ps1 -Population 32 -Generations 10
```

### ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ç›´æ¥å®Ÿè¡Œ
```bash
python evolution.py \
  --experiment_name "code_nas_v1_single" \
  --population 24 \
  --generations 8 \
  --use_real_training \
  --train_path "../data/code_char/train.txt" \
  --val_path "../data/code_char/val.txt" \
  --seq_len 256 \
  --batch_size 32 \
  --max_train_steps 300 \
  --device "cuda:0" \
  --search_mode "medium"
```

### æœŸå¾…ã•ã‚Œã‚‹çµæœ
- å®Ÿè¡Œæ™‚é–“: 1.5ã€œ3æ™‚é–“
- æœŸå¾…Fitness: 1.0
- å‡ºåŠ›: `logs/code_nas_v1_single/evolution/best_architecture.json`

### å®Ÿæ¸¬çµæœ (2024-12)

| æŒ‡æ¨™ | å€¤ |
|------|-----|
| Best Fitness | 1.0000 |
| Val Loss | 0.0188 |
| Val PPL | 1.02 |
| Accuracy | 98.14% |
| Params | 2.68M |
| Model Size | 3.06 MB |
| Latency | 3.0 ms |
| Evaluated | 144 archs (6 gen) |
| Runtime | ~15-20 min |

**Best Architecture:**
- `models/codenas_v1_best_transformer.json`

```
arch_type: transformer
num_layers: 4
hidden_dim: 256
num_heads: 8
ffn_multiplier: 3.0
normalization: rmsnorm
activation: gelu
position_encoding: rope
```

> çŸ­ã„ã‚³ãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆ1.3MB accelerateï¼‰+ 300 steps + å°ãƒ¢ãƒ‡ãƒ«å„ªä½ã®ãŸã‚äºˆæƒ³ã‚ˆã‚ŠçŸ­æ™‚é–“ã§å®Œäº†ã€‚
> Gen 0ã§æ—¢ã«fitness=1.0é”æˆã€ä»¥é™ã‚‚åŒç­‰ã®ç²¾åº¦ã‚’ç¶­æŒã€‚

---

## 5.5 CodeNAS v2 (Two-stage NAS)

**Multi-fidelity NAS: Stage 1ã§ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° â†’ Stage 2ã§Top-kç²¾å¯†è©•ä¾¡**

### è¨­å®šå€¤
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ | èª¬æ˜ |
|-----------|-----|------|
| population | 24 | ä¸–ä»£ã‚ãŸã‚Šã®å€‹ä½“æ•° |
| generations | 8 | é€²åŒ–ä¸–ä»£æ•° |
| search_mode | medium | æ¢ç´¢ç©ºé–“ |
| two_stage | true | 2æ®µéšè©•ä¾¡æœ‰åŠ¹ |
| stage1_steps | 50 | ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå°‘ã‚¹ãƒ†ãƒƒãƒ—ï¼‰ |
| stage2_steps | 300 | ç²¾å¯†è©•ä¾¡ï¼ˆå¤šã‚¹ãƒ†ãƒƒãƒ—ï¼‰ |
| top_k | 6 | Stage 2ã«é€²ã‚€å€™è£œæ•° |
| seq_len | 256 | ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· |
| batch_size | 32 | ãƒãƒƒãƒã‚µã‚¤ã‚º |
| device | cuda:0 | ä½¿ç”¨GPU |

### PowerShellå®Ÿè¡Œï¼ˆæ¨å¥¨ï¼‰
```powershell
cd nas\scripts
.\run_codenas_v2_two_stage.ps1

# ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŒ‡å®š
.\run_codenas_v2_two_stage.ps1 -Population 32 -Generations 10 -TopK 8
```

### ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œï¼ˆç›´æ¥ï¼‰
```bash
python evolution.py \
  --experiment_name "code_nas_v2_two_stage" \
  --population 24 \
  --generations 8 \
  --use_real_training \
  --train_path "../data/code_char/train.txt" \
  --val_path "../data/code_char/val.txt" \
  --seq_len 256 \
  --batch_size 32 \
  --device "cuda:0" \
  --search_mode "medium" \
  --two_stage \
  --stage1_steps 50 \
  --stage2_steps 300 \
  --top_k 6
```

### æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ
- Stage 1 (50 steps): 24å€™è£œã‚’é«˜é€Ÿã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° â†’ ç´„1åˆ†/ã‚¢ãƒ¼ã‚­
- Stage 2 (300 steps): Top 6ã®ã¿ç²¾å¯†è©•ä¾¡ â†’ ç´„3-5åˆ†/ã‚¢ãƒ¼ã‚­
- v1æ¯”: è©•ä¾¡æ™‚é–“å‰Šæ¸› (24Ã—300 â†’ 24Ã—50 + 6Ã—300 = 3000 steps vs 7200 steps = 58%å‰Šæ¸›)

### æ¯”è¼ƒãƒ„ãƒ¼ãƒ«
```bash
# v1 vs v2ã‚’æ¯”è¼ƒï¼ˆå¼•æ•°ãªã—ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¯”è¼ƒï¼‰
python compare_experiments.py

# ã¾ãŸã¯æ˜ç¤ºçš„ã«æŒ‡å®š
python compare_experiments.py logs/code_nas_v1_single logs/code_nas_v2_two_stage
```

### å®Ÿæ¸¬çµæœ (2024-12)

| ãƒ¡ãƒˆãƒªãƒƒã‚¯ | å€¤ | å‚™è€ƒ |
|-----------|-----|------|
| **Best Fitness** | 1.0000 | âœ… å®Œç’§ãªã‚¹ã‚³ã‚¢ |
| **Architecture** | Transformer L4 H384 | Heads=4, FFNÃ—2.0, SiLU, RoPE |
| **Parameters** | 4.80M | v1 (2.68M) ã‚ˆã‚Šå¤§ãã„ |
| **Model Size** | 7.32 MB | v1 (3.06 MB) ã‚ˆã‚Šå¤§ãã„ |
| **Val Loss** | 0.0142 | v1 (0.0188) ã‚ˆã‚Š**è‰¯ã„** |
| **Val PPL** | 1.01 | v1 (1.02) ã‚ˆã‚Šè‰¯ã„ |
| **Accuracy** | 98.59% | v1 (98.14%) ã‚ˆã‚Š0.45%é«˜ã„ |
| **Latency** | 3.53 ms | v1 (3.01 ms) ã‚ˆã‚Šé…ã„ |
| **Train Time** | 3.52 s | v1 (5.03 s) ã‚ˆã‚Šé€Ÿã„ |
| **Generations** | 8 | Population=24, Stage1=50, Stage2=300 |

#### v1 vs v2 æ¯”è¼ƒ

```bash
python compare_experiments.py
```

**çµè«–**:
- v2ã¯**ç²¾åº¦ã§v1ã‚’ä¸Šå›ã‚‹**ï¼ˆVal Loss 0.0142 vs 0.0188ï¼‰
- ã—ã‹ã—**ã‚µã‚¤ã‚ºã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã§ã¯v1ãŒå„ªã‚Œã‚‹**ï¼ˆ3.06MB vs 7.32MBï¼‰
- Two-stage NASã¯ç²¾åº¦é‡è¦–ã®æ¢ç´¢ã«æœ‰åŠ¹
- **å®Ÿç”¨çš„ã«ã¯è»½é‡æ€§ã‚’é‡è¦–ã—ã¦v1ã‚’æ¨å¥¨**

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¯è¦–åŒ–

```bash
# Current best model (v1)
python visualize_architecture.py models/codenas_best_current.json

# v1 original
python visualize_architecture.py logs/code_nas_v1_single/evolution/best_architecture.json

# v2 two-stage
python visualize_architecture.py logs/code_nas_v2_two_stage/evolution/best_architecture.json
```

---

## 5.6 v1 Production Training (Full Training)

**æœ¬å‘½ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆv1ï¼‰ã®æœ¬æ ¼è¨“ç·´**

### è¨­å®šå€¤
| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å€¤ | èª¬æ˜ |
|-----------|-----|------|
| architecture | v1 single-stage | L4 H256 Heads=8 FFNÃ—3.0 |
| max_steps | 10,000 | æœ¬ç•ªç”¨é•·æ™‚é–“è¨“ç·´ |
| learning_rate | 3e-4 â†’ 1e-5 | Cosine decay with warmup |
| warmup_steps | 500 | LR warmup |
| batch_size | 32 | |
| seq_len | 256 | |
| device | cuda:0 | RTX 5090 |

### ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
```bash
python train_best.py \
  --arch_json models/codenas_best_current.json \
  --experiment_name v1_production \
  --max_steps 10000 \
  --log_dir logs/train_v1_production
```

### å®Ÿæ¸¬çµæœ (2024-12)

| ãƒ¡ãƒˆãƒªãƒƒã‚¯ | å€¤ | å‚™è€ƒ |
|-----------|-----|------|
| **Final Val Loss** | 0.0065 | Best: 0.0065 |
| **Final Val PPL** | 1.01 | éå¸¸ã«ä½ã„ |
| **Parameters** | 2.68M | |
| **Model Size** | 5.10 MB | æ¨å®šå€¤ |
| **Latency** | 2.97 ms | RTX 5090 |
| **Training Time** | 9.6 min | 10,000 steps |
| **Steps/sec** | 18.5 (avg) | åˆæœŸ56â†’å¾ŒåŠ18 |
| **Checkpoint** | v1_production_best.pt | |

### å­¦ç¿’æ›²ç·š
- Step 100: Loss 3.56 â†’ PPL 35.11
- Step 500: Loss 0.0175 â†’ PPL 1.02 (warmupå®Œäº†)
- Step 1000: Loss 0.0098 â†’ PPL 1.01
- Step 5000: Loss 0.0069 â†’ PPL 1.01
- Step 10000: Loss 0.0053 â†’ PPL 1.01 âœ…

### Playgroundãƒ†ã‚¹ãƒˆ
```bash
python eval_playground.py
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ« (v1_production_best.pt) ã‚’ãƒ­ãƒ¼ãƒ‰
```

**ç”Ÿæˆå“è³ª**: é™å®šçš„ï¼ˆå˜ç´”ãªãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆï¼‰
- ç†ç”±: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãŒå°ã•ã„ï¼ˆ1.3MB Python codeï¼‰
- æ”¹å–„ç­–: ã‚ˆã‚Šå¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚ˆã‚Šé•·ã„è¨“ç·´

---

## 6. Sanity Check (ä¸¦åˆ— vs éä¸¦åˆ—)

> **çµæœ**: PASSED (2024-12) - Sequential: 1.0, Parallel: 1.0

```bash
cd nas
bash scripts/sanity_check.sh
```

çµæœç¢ºèª:
```bash
# Sequentialçµæœ
cat logs/sanity_seq/evolution/best_architecture.json | python -c "import sys,json; d=json.load(sys.stdin); print(f'Fitness: {d[\"fitness\"]:.4f}')"

# Parallelçµæœ
cat logs/sanity_par/evolution/best_architecture.json | python -c "import sys,json; d=json.load(sys.stdin); print(f'Fitness: {d[\"fitness\"]:.4f}')"
```

---

## 7. Analysis Tools

### ä¸¦åˆ—ãƒ­ã‚°è§£æ
```bash
# åŸºæœ¬è§£æ
python analyze_parallel_stats.py --log_dir logs/medium_nas/parallel

# 2ã¤ã®å®Ÿé¨“ã‚’æ¯”è¼ƒ
python analyze_parallel_stats.py \
  --log_dir logs/single_gpu_exp/parallel \
  --compare logs/dual_gpu_exp/parallel
```

### GPUæ ¡æ­£ï¼ˆå®Ÿæ¸¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰
```bash
python calibrate_gpus.py --num_runs 3 --output logs/gpu_calibration.json
```

### çµæœç¢ºèª
```bash
# Best architecture
cat logs/medium_nas/evolution/best_architecture.json | python -m json.tool

# Fitness history
cat logs/medium_nas/evolution/fitness_history.json | python -c "
import sys, json
data = json.load(sys.stdin)
for h in data:
    print(f\"Gen {h['generation']}: best={h['best_fitness']:.4f}, mean={h['mean_fitness']:.4f}\")
"
```

---

## 8. GPU Status Check

```bash
# GPUçŠ¶æ…‹ç¢ºèª
nvidia-smi

# CUDAåˆ©ç”¨å¯èƒ½ç¢ºèª
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}')"

# è©³ç´°GPUæƒ…å ±
python -c "
import torch
for i in range(torch.cuda.device_count()):
    p = torch.cuda.get_device_properties(i)
    print(f'GPU {i}: {p.name} ({p.total_memory/1e9:.1f} GB)')
"
```

---

## 9. Directory Structure

```
nas/
â”œâ”€â”€ evolution.py          # ãƒ¡ã‚¤ãƒ³NASã‚¨ãƒ³ã‚¸ãƒ³
â”œâ”€â”€ parallel_evaluator.py # ãƒãƒ«ãƒGPUè©•ä¾¡å™¨
â”œâ”€â”€ evaluator.py          # ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è©•ä¾¡
â”œâ”€â”€ fitness.py            # é©å¿œåº¦é–¢æ•°
â”œâ”€â”€ search_space.py       # æ¢ç´¢ç©ºé–“
â”œâ”€â”€ train_loop.py         # è¨“ç·´ãƒ«ãƒ¼ãƒ—
â”œâ”€â”€ analyze_parallel_stats.py  # ãƒ­ã‚°è§£æ
â”œâ”€â”€ calibrate_gpus.py     # GPUæ ¡æ­£
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ sanity_check.sh
â”‚   â”œâ”€â”€ run_dual_gpu.ps1
â”‚   â”œâ”€â”€ run_dual_gpu_quick_test.ps1
â”‚   â””â”€â”€ run_dual_gpu_production.ps1
â””â”€â”€ logs/
    â””â”€â”€ <experiment_name>/
        â”œâ”€â”€ evolution/
        â”‚   â”œâ”€â”€ best_architecture.json
        â”‚   â”œâ”€â”€ fitness_history.json
        â”‚   â”œâ”€â”€ fitness_history.png
        â”‚   â””â”€â”€ checkpoint_gen*.json
        â””â”€â”€ parallel/
            â”œâ”€â”€ parallel_worker_stats.json
            â”œâ”€â”€ parallel_batch_stats.json
            â””â”€â”€ worker_*_cuda_*/
```

---

## 10. Troubleshooting

### OOM (Out of Memory)
```bash
# ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ä¸‹ã’ã‚‹
--batch_size 8

# ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’åˆ¶é™ï¼ˆsearch_mode=minimalã§å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã®ã¿ï¼‰
--search_mode "minimal"
```

### CUDA Error
```bash
# GPUãƒªã‚»ãƒƒãƒˆ
nvidia-smi --gpu-reset

# ãƒ—ãƒ­ã‚»ã‚¹ç¢ºèªãƒ»çµ‚äº†
nvidia-smi
kill -9 <PID>
```

### Parallel Evaluation Timeout
```python
# evolution.pyã§èª¿æ•´
max_eval_time_s=7200.0  # 2 hours
```

---

## 11. Expected Results

| è¨­å®š | æœŸå¾…Fitness | æœŸå¾…æ™‚é–“ |
|------|-------------|----------|
| Smoke test | 0.5-0.9 | 2-3åˆ† |
| Medium run | 0.9-1.0 | 30-40åˆ† |
| Production (1GPU) | 1.0 | 2-4æ™‚é–“ |
| Production (2GPU) | 1.0 | 1-2æ™‚é–“ |

---

## 12. BigData Training (ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µå®Ÿé¨“)

**ç›®çš„**: ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’ 1.3MB â†’ 100MB+ ã«å¢—ã‚„ã—ã¦ã€ãƒ¢ãƒ¼ãƒ‰å´©å£Šå•é¡Œã‚’è§£æ±ºã™ã‚‹ã€‚

### Step 1: å¤§è¦æ¨¡Pythonã‚³ãƒ¼ãƒ‘ã‚¹ã‚’æº–å‚™

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³A: The Stack dataset (æ¨å¥¨)

```bash
# HuggingFace The Stackã‹ã‚‰ Pythonã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# https://huggingface.co/datasets/bigcode/the-stack

# ä¾‹: 100MBã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (æ‰‹å‹•ã¾ãŸã¯HF datasetsãƒ©ã‚¤ãƒ–ãƒ©ãƒª)
# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆ: data/raw_python/
```

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³B: CodeSearchNet dataset

```bash
# https://github.com/github/CodeSearchNet
# Pythonéƒ¨åˆ†ã®ã¿æŠ½å‡º: data/raw_python/
```

#### ã‚ªãƒ—ã‚·ãƒ§ãƒ³C: GitHubäººæ°—ãƒªãƒã‚¸ãƒˆãƒª

```bash
# ä¾‹: requests, flask, django, numpy, pandas, scikit-learn
cd data/raw_python
git clone https://github.com/psf/requests.git
git clone https://github.com/pallets/flask.git
git clone https://github.com/django/django.git
# ãªã©...
```

### Step 2: ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ char-level ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›

```bash
cd nas

# ç”Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ train/val ã‚’ç”Ÿæˆ
python scripts/prepare_python_corpus.py \
  --src_dir ../data/raw_python \
  --train_out ../data/code_char_big/train.txt \
  --val_out ../data/code_char_big/val.txt \
  --val_ratio 0.01 \
  --min_file_size 100 \
  --max_file_size 262144

# å‡ºåŠ›ä¾‹:
# [COLLECT] OK Collected 12,345 valid Python files
# [WRITE] OK Train corpus: 12,222 files, 1,234,567 lines, 123.45 MB
# [WRITE] OK Val corpus: 123 files, 12,345 lines, 1.23 MB
```

**æœŸå¾…ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º**:
- æœ€ä½: 50MB (å°è¦æ¨¡æ”¹å–„æœŸå¾…)
- æ¨å¥¨: 100-500MB (å¤§å¹…æ”¹å–„æœŸå¾…)
- ç†æƒ³: 1GB+ (GPT-2ãƒ¬ãƒ™ãƒ«ã®å“è³ªæœŸå¾…)

### Step 3: v1ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§å†å­¦ç¿’

```bash
cd nas

# 100K steps ã§å­¦ç¿’ (ç¾çŠ¶ã®10å€)
python train_best.py \
  --arch_json models/codenas_best_current.json \
  --experiment_name v1_bigdata_char \
  --train_path ../data/code_char_big/train.txt \
  --val_path ../data/code_char_big/val.txt \
  --max_steps 100000 \
  --log_dir logs/train_v1_bigdata_char \
  --device cuda:0

# ãƒ­ã‚°:
# - Checkpoint: logs/train_v1_bigdata_char/v1_bigdata_char_best.pt
# - TensorBoard: logs/train_v1_bigdata_char/events.out.tfevents.*
```

**å­¦ç¿’æ™‚é–“ (æ¦‚ç®—)**:
- 50MB ãƒ‡ãƒ¼ã‚¿: ~30-60åˆ† (100K steps)
- 100MB ãƒ‡ãƒ¼ã‚¿: ~60-120åˆ†
- 500MB ãƒ‡ãƒ¼ã‚¿: ~3-6æ™‚é–“
- 1GB ãƒ‡ãƒ¼ã‚¿: ~6-12æ™‚é–“

### Step 4: è©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§å†æ¤œè¨¼

```bash
cd nas

# ãƒãƒƒãƒè©•ä¾¡
python eval_playground.py \
  --checkpoint logs/train_v1_bigdata_char/v1_bigdata_char_best.pt \
  --eval_file eval/prompts/simple_python.txt \
  --output eval/results_bigdata.jsonl

# è§£æ
python eval/inspect_results.py eval/results_bigdata.jsonl --show_quality_examples

# æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„:
# - Mode collapseç‡: 85.2% â†’ <30%
# - Python keywordså‡ºç¾ç‡: 0% â†’ >50%
# - å¹³å‡repetitionæ¯”ç‡: 92.22% â†’ <30%
```

### Step 5: çµæœã‚’ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åæ˜ 

```bash
# EVALUATION_SUMMARY.md ã« BigData ç‰ˆã®çµæœã‚’è¿½è¨˜
# README.md ã® Phase 3 ã«é€²æ—ã‚’æ›´æ–°
```

---

## 13. æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ— (BigData å®Ÿé¨“å¾Œ)

### BigData å®Ÿé¨“ãŒæˆåŠŸã—ãŸå ´åˆ
â†’ ã•ã‚‰ã«ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã—ã¦å“è³ªã‚’å‘ä¸Š
â†’ Token-level modeling ã«ç§»è¡Œã—ã¦åŠ¹ç‡åŒ–

### BigData å®Ÿé¨“ã§ã‚‚æ”¹å–„ãŒä¸ååˆ†ãªå ´åˆ
â†’ Token-level modeling (BPE/SentencePiece) ã«åˆ‡ã‚Šæ›¿ãˆ
â†’ Knowledge distillation (GPT-4 â†’ student model)

---

## 14. BigData Training (ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µå®Ÿé¨“) ğŸ”¥ RECOMMENDED

**Status**: Token-level infrastructure complete, ready for large-scale data
**Goal**: Mode collapseè§£æ±º (85% â†’ <10%) via 100MB-1GB Python corpus
**Hardware**: RTX 5090 (heavy training OK)

### Step 1: ã‚³ãƒ¼ãƒ‘ã‚¹æº–å‚™ (Char + Token ä¸¡å¯¾å¿œ)

```bash
cd nas

# Option A: æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å†åˆ©ç”¨ (ãƒ†ã‚¹ãƒˆç”¨ã€7.92MB)
python scripts/prepare_python_corpus.py \
  --src_dir ../data/raw_python \
  --char_train ../data/code_char_big/train.txt \
  --char_val   ../data/code_char_big/val.txt \
  --token_train ../data/code_token_big/train.txt \
  --token_val   ../data/code_token_big/val.txt \
  --mode both \
  --val_ratio 0.01

# Option B: å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿åé›† (æœ¬ç•ªç”¨ã€100MB-1GB)
# 1. ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
cd ../data/raw_python

# The Stack dataset (æ¨å¥¨ã€é«˜å“è³ªPython corpus)
# https://huggingface.co/datasets/bigcode/the-stack
# ã¾ãŸã¯ GitHub repos ã‚’ç›´æ¥clone:
git clone --depth 1 https://github.com/psf/requests.git
git clone --depth 1 https://github.com/pallets/flask.git
git clone --depth 1 https://github.com/django/django.git
git clone --depth 1 https://github.com/scikit-learn/scikit-learn.git
git clone --depth 1 https://github.com/pandas-dev/pandas.git
git clone --depth 1 https://github.com/numpy/numpy.git
# ... (15-20 repos ã§ 100MB é”æˆå¯èƒ½)

# 2. ã‚³ãƒ¼ãƒ‘ã‚¹ç”Ÿæˆ (target_size_mb ã§å®¹é‡åˆ¶é™)
cd ../../nas
python scripts/prepare_python_corpus.py \
  --src_dir ../data/raw_python \
  --char_train ../data/code_char_bigdata/train.txt \
  --char_val   ../data/code_char_bigdata/val.txt \
  --token_train ../data/code_token_bigdata/train.txt \
  --token_val   ../data/code_token_bigdata/val.txt \
  --mode both \
  --val_ratio 0.01 \
  --target_size_mb 500 \
  --max_file_size 524288
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**:
```
CHAR-LEVEL:
  Total:  500 MB, 15M lines
  Train:  495 MB (10,000 files)
  Val:    5 MB (100 files)

TOKEN-LEVEL:
  Total:       500 MB, 250M tokens
  Compression: 2.0x (chars/tokens)
  Vocab size:  50,257 (gpt2)
```

### Step 2: Token-level BigData å®Ÿé¨“ (æ¨å¥¨) â­â­â­â­â­

```bash
cd nas

# ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ (10K steps, ~1æ™‚é–“)
python train_best.py \
  --arch_json models/codenas_best_current.json \
  --experiment_name v1_token_bigdata_smoke \
  --train_path ../data/code_token_bigdata/train.txt \
  --val_path   ../data/code_token_bigdata/val.txt \
  --max_steps 10000 \
  --warmup_steps 500 \
  --use_tokens \
  --log_dir logs/train_v1_token_bigdata_smoke \
  --device cuda:0

# æœ¬ç•ªè¨“ç·´ (100K steps, ~10-20æ™‚é–“ã€RTX 5090ã§é«˜é€Ÿ)
python train_best.py \
  --arch_json models/codenas_best_current.json \
  --experiment_name v1_token_bigdata_production \
  --train_path ../data/code_token_bigdata/train.txt \
  --val_path   ../data/code_token_bigdata/val.txt \
  --max_steps 100000 \
  --warmup_steps 2000 \
  --lr 3e-4 \
  --min_lr 1e-5 \
  --use_tokens \
  --log_dir logs/train_v1_token_bigdata_production \
  --device cuda:0

# è©•ä¾¡
python eval_playground.py \
  --checkpoint logs/train_v1_token_bigdata_production/v1_token_bigdata_production_best.pt \
  --eval_file eval/prompts/simple_python.txt \
  --output eval/results_token_bigdata.jsonl \
  --mode token

python eval/inspect_results.py eval/results_token_bigdata.jsonl --show_quality_examples
```

**æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„**:
- Mode collapse: 100% â†’ <10%
- Python keywords: 0% â†’ >90%
- Val PPL: 1.036 â†’ <1.01
- ç”Ÿæˆå“è³ª: Repetitive â†’ Coherent code

### Step 3: Char-level BigData å®Ÿé¨“ (å‚è€ƒã€éæ¨å¥¨)

```bash
# Char-level ã¯ 100MB ã§ã‚‚ä¸ååˆ†ãªå¯èƒ½æ€§ãŒé«˜ã„
# Token-level æ¨å¥¨

cd nas

python train_best.py \
  --arch_json models/codenas_best_current.json \
  --experiment_name v1_char_bigdata \
  --train_path ../data/code_char_bigdata/train.txt \
  --val_path   ../data/code_char_bigdata/val.txt \
  --max_steps 100000 \
  --log_dir logs/train_v1_char_bigdata \
  --device cuda:0
```

### Step 4: æ¯”è¼ƒåˆ†æ

```bash
cd nas

# Token-level vs Char-level æ¯”è¼ƒ
python compare_training_runs.py \
  logs/train_v1_token_bigdata_production \
  logs/train_v1_char_bigdata
```

### å®Ÿé¨“ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¨å¥¨å€¤

| Dataset Size | Char-level Steps | Token-level Steps | Training Time (RTX 5090) |
|--------------|------------------|-------------------|--------------------------|
| 7.92 MB | 5,000 | 5,000 | ~10 min |
| 50 MB | 50,000 | 20,000 | ~2-4 hours |
| 100 MB | 100,000 | 30,000 | ~4-8 hours |
| 500 MB | 500,000 | 100,000 | ~20-40 hours |
| 1 GB | 1,000,000 | 200,000 | ~40-80 hours |

**Note**: Token-level ã¯ Char-level ã® 1/3-1/5 ã® steps ã§åŒç­‰å“è³ªã«åˆ°é”

### ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

**Q: ãƒ‡ãƒ¼ã‚¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒé…ã„**
A: GitHub repos ã® shallow clone ã‚’ä½¿ã† (`--depth 1`)

**Q: ãƒ¡ãƒ¢ãƒªä¸è¶³**
A: `--max_file_size` ã‚’ä¸‹ã’ã‚‹ã€ã¾ãŸã¯ `--target_size_mb` ã‚’å°ã•ãã™ã‚‹

**Q: Mode collapse ãŒè§£æ±ºã—ãªã„**
A: ã•ã‚‰ã«ãƒ‡ãƒ¼ã‚¿é‡ã‚’å¢—ã‚„ã™ (500MB â†’ 1GB)ã€ã¾ãŸã¯ Knowledge Distillation (Option C) ã‚’è©¦ã™

---

*Last updated: 2025-12-09*
