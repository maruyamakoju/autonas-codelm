{
  "experiment_name": "v1_sanity_300",
  "final_metrics": {
    "val_loss": 0.010630391565154645,
    "val_ppl": 1.0106870949257578,
    "best_val_loss": Infinity,
    "num_params": 18198016,
    "model_size_mb": 27.2099609375,
    "latency_ms": 10.203523635864258,
    "total_steps": 300,
    "train_time_s": 64.0444974899292,
    "train_time_min": 1.0674082914988199,
    "architecture": {
      "arch_type": "transformer",
      "num_layers": 6,
      "hidden_dim": 512,
      "num_heads": 8,
      "ffn_multiplier": 2.5,
      "normalization": "rmsnorm",
      "activation": "swiglu",
      "position_encoding": "rope",
      "attention_dropout": 0.1,
      "residual_dropout": 0.1,
      "vocab_size": 101,
      "max_seq_length": 256,
      "quantization": "fp16",
      "pruning_ratio": 0.0
    }
  },
  "total_steps": 3,
  "all_metrics": [
    {
      "step": 100,
      "timestamp": 1765181166.6797974,
      "train_loss": 1.9382456943020225,
      "train_ppl": 6.946553895985589,
      "lr": 0.00027230746418436733,
      "grad_norm": 0.22398309409618378,
      "steps_per_sec": 26.657472178995064
    },
    {
      "step": 200,
      "timestamp": 1765181170.0748844,
      "train_loss": 0.020852396003901957,
      "train_ppl": 1.0210713263059077,
      "lr": 0.00011019253581563262,
      "grad_norm": 0.0698036178946495,
      "steps_per_sec": 27.98620768297835
    },
    {
      "step": 300,
      "timestamp": 1765181173.4801092,
      "train_loss": 0.01135189548600465,
      "train_ppl": 1.0114165727564115,
      "lr": 1e-05,
      "grad_norm": 0.05722641944885254,
      "steps_per_sec": 28.43167561624105
    }
  ]
}